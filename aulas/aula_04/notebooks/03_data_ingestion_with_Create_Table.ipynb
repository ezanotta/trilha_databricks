{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "125b02ed-7af5-4c67-8649-7ff638c1db31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Este notebook faz a ingestão dos dados com CREATE TABLE AS \n",
    "\n",
    "## Volumes a serem criados:\n",
    "- `claims` - Volume para dados e imagens de sinistros (CSV, imagens, metadata)\n",
    "- `sql_server` - Volume para dados extraídos do SQL Server (CSV: claims, customers, policies)\n",
    "- `telematics` - Volume para dados de telemetria veicular (arquivos Parquet)\n",
    "- `training_imgs` - Volume para imagens de treinamento de modelos de Machine Learning (PNG)\n",
    "\n",
    "## O que são Volumes no Unity Catalog?\n",
    "- **Volumes** são containers de arquivos no Unity Catalog\n",
    "- Permitem armazenar arquivos não estruturados ou semi-estruturados (CSV, JSON, Parquet, imagens, etc.)\n",
    "- Fornecem governança e controle de acesso granular\n",
    "- São a alternativa moderna ao DBFS para armazenamento de arquivos\n",
    "- Permitem organizar dados por tipo ou origem antes do processamento\n",
    "\n",
    "## Nota sobre Upload\n",
    "Após criar os volumes neste notebook, você pode fazer upload dos arquivos diretamente via **UI do Databricks**:\n",
    "1. Navegue até: Catalog → smart_claims_dev → 00_landing\n",
    "2. Clique no volume desejado\n",
    "3. Clique em **Upload** ou **Add files**\n",
    "4. Selecione os arquivos/pastas do seu sistema local\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cf8abe8-7367-4419-a434-1438e89842ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Primeiro, vamos garantir que estamos usando o catálogo correto e que o schema 00_landing existe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cee20a5-8260-433d-bf33-4c5f91e15d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT current_catalog(), current_schema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db5cae5d-f95e-4562-8185-be717eebaa9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## EXPLORE THE DATA SOURCE FILES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c815259d-32c9-42ac-831a-eecdb8c1e476",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LIST /Volumes/smart_claims_dev.00_landing/sql_server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query os arquivos parquet usando o path do volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM parquet.`/Volumes/smart_claims_dev.00_landing/sql_server/claims.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdb4954a-0852-412d-879c-0ef22ac77f82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Batch Data Ingestion with CTAs\n",
    "\n",
    "CTAs with CREATE TABLE AS é uma maneira eficiente de ingerir dados em massa.\n",
    "\n",
    "Dentro do Unity Catalog, podemos criar tabelas Delta a partir de arquivos em volumes.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82e0ad1f-e244-44cf-9aee-4b873b359eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "SELECT *\n",
    "FROM read_files(\n",
    "    'dbfs:/Volumes/smart_claims_dev.00_landing/sql_server/claims.csv',\n",
    "    format = 'csv'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--drop the table if it exists for demonstration purposes\n",
    "DROP TABLE IF EXISTS smart_claims_dev.01_bronze.claims;\n",
    "\n",
    "--create the table using the CREATE TABLE AS statement\n",
    "CREATE TABLE smart_claims_dev.01_bronze.claims\n",
    "AS SELECT *\n",
    "FROM read_files(\n",
    "    'dbfs:/Volumes/smart_claims_dev.00_landing/sql_server/claims.csv',\n",
    "    format = 'csv'\n",
    ")\n",
    "\n",
    "-- Preview the data\n",
    "SELECT *\n",
    "FROM smart_claims_dev.01_bronze.claims\n",
    "LIMIT 10\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESCRIBE TABLE EXTENDED smart_claims_dev.01_bronze.claims;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%python\n",
    "df = (spark.\n",
    "        read.\n",
    "        format(\"csv\").\n",
    "        option(\"header\", True).\n",
    "        load(\"dbfs:/Volumes/smart_claims_dev.00_landing/sql_server/claims.csv\")\n",
    ")\n",
    "\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"smart_claims_dev.01_bronze.claims\")\n",
    "\n",
    "claims_table = spark.table(\"smart_claims_dev.01_bronze.claims\")\n",
    "\n",
    "claims_table.display()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COPY INTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP TABLE IF EXISTS smart_claims_dev.01_bronze.claims;\n",
    "\n",
    "CREATE TABLE smart_claims_dev.01_bronze.claims (\n",
    "    claim_no STRING,\n",
    "    policy_no STRING,\n",
    "    claim_date DATE,\n",
    "    months_as_customer INT,\n",
    "    injury STRING,\n",
    "    property STRING,\n",
    "    vehicle STRING,\n",
    ")\n",
    "\n",
    "COPY INTO smart_claims_dev.01_bronze.claims\n",
    "FROM 'dbfs:/Volumes/smart_claims_dev.00_landing/sql_server/claims.csv'\n",
    "FILEFORMAT = CSV\n",
    "COPY_OPTIONS (header = true, infer_schema = true)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_create_volumes_and_load_data",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
